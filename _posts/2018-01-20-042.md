---
layout: post
title: metatorium
category: pop up windows
categoryLink: pop_up_windows
day: january 20th
city: New York City
number: 42
year: 2018
comments: true
semester: spring 18
#projectSemester: _season year

#Cities
#	New York City
#	Barcelona

#categories:
#	applications
#	physical computing 
#	computational media 
#	sound & video 
#	alt docs
#	towers of power 
#	analog circuits 
#	nature of code
#	networked media
#	piecing it together
#	bluetooth LE
#	live web
#	understanding networks
#	pop up windows
#	game design & psychology
#	thesis
#	homemade hardware

---

_This is a retrospective, AKA postmortem of what was it like to create **Metaroruim.** Done for a class at **ITP** called **"Pop-up Window Displays"**. Metatorium was located at the [NYU Broadway Windows](https://www.google.com/maps/place/40°43'54.9%22N+73°59'29.7%22W/@40.731923,-73.992312,18z/data=!3m1!4b1!4m6!3m5!1s0x0:0x0!7e2!8m2!3d40.7319232!4d-73.9915898)._

_A project done alongside 3 other people; [Regina Cantú](http://www.reginacantu.com/), [Alexia Kyriakopoulos](http://www.alexiak.com), and [Ari Meliciano](http://www.ariciano.com). It goes without saying that this post is from my point of view of the experience._

---

# Origin Story

The main idea behind this was an observation post, a place where people could observe themselves and others, we wanted to play with the obsession most people unconsciously have towards themselves.

With that in mind I came up with a basic prototype to be able to test out the interaction.

![](/img/thumnailsBlog/42_8.gif)

For the last couple months I had personally been very interested in face detection, but the project to apply this never came, until this one. it was a good chance to try it.

A screen was set up to show a mirrored image of a webcam footage, this way people would see themselves and maybe, react to it. The footage above is a portion of the images taken during said test. This was not only a mirror, but a **python** script that with [openCV]() would take captures every _x_ amount of seconds.

From this captures, **openCV** would look for faces, and if one was found, it would crop it and save it in a different folder.

![](/img/thumnailsBlog/42_9.gif)

_I am fully aware of the absence of any kind of technical explanation here. [Soon enough]() that will be fixed. I will explain the very basics of **openCV** and how we made use of it for the setup we had._

## Success!

The test was very successful, people flocked around it, even if nothing was apparently happening, they would still be there. I indeed took images and cropped them but that first time I didn't do anything with them.

We realized that we needed some kind of response/reward for being there, besides just seeing your face in a big screen. So we made use of those cropped images.

The next time we set up the application a window was added showing the last image cropped. For obvious reasons the results were even better.

![](/img/thumnailsBlog/42_10.gif)

Surely, people used the "mirror" even more, and we observed that they would come back to have their face on the screen again.

At this point, what was being used to display the last faced that had been cropped, was a new window created with **openCV**, that showed to be quite problematic and the main cause for _crashing the script when more than 1 face was found_ in a picture taken.

![](/img/thumnailsBlog/42_11.gif)

Said crash would not stop the whole process, but only part if ot, meaning that it would get stuck in a loop checking for faces in the same capture.

This was not the only problem we had, the detection was not precise enough, it would get stuck thinking that a piece of furniture or a smudge in the floor were faces. _This was adjusted through the settings, but there was always that sneaky shadow that would still be there._

# Building it

Now we knew how the interaction was, so the work was needed for this interaction was going to be shown.

Coming back to the idea of an observation post, lost of plausible representations came to mind, but the one that stuck, was the vision of a **plant-like organism** that would feed itself from faces, meaning that instead of sunlight, this plant would need people and their faces. The aura of this was a _tech induced plant_. With iridescent leafs, light producing branches, and screens as flowers.

![](/img/thumnailsBlog/42_4.png){: .proj_img1024}
_This is the shape it had on the day of the opening._{: .caption1024}

## "Front end"

This plant-like thing that has screens as flowers, needs a lot of them, and a Mac Mini can handle so many. _That meant having 1 mac mini for every 2 displays_. At one point we were planing to have up to 8 screens but that happened to be obviously too much. We needed to figure out two things, both of them equally important, the first one, the content in each of the screens, and the second one, how to arrange these.

To figure out the arrangement we set up a space to be a placeholder for the window-to-be. That way we were able to start playing around with the different possibilities.

![](/img/thumnailsBlog/42_12.png)
*the "mock-up" area*{: .caption65}

![](/img/thumnailsBlog/42_13.png)
*testing different configurations*{: .caption65}

After a while, we came to the conclusion that we needed/could fit **6 screens** in the space we had. These would be placed with a **speed-rail structure** that later was hidden with black foamcore. Once this was done we needed to decide the what the contents of each screen would be.

Knowing that the central element was the mirror, it was clear that the main monitor would be it. After that, we had two more things that had to be shown, one was the last face that got taken, and the other one was an array of some of the other collected faces before the last one. The final content was the on shown in the following section. Besides the content of the screens, something else that had to be resolved was the communication between the three Mac Minis that we where gonna use.

## "Back end"

Here we talk about how the screens and their respective computers talked to each other so the content would show as we wanted it to be.
_I'm fairly sure that there are way better ways of doing this and that this is not "long-term-proof", but we had two weeks to make it work, and we went with what we had_

![](/img/thumnailsBlog/42_7.svg)

This diagram has the **6** screens represented, each of them has a different **number (1-6)** and each of the **3** computers is represented by a **letter (A-C)**.

Computer **A** it's the main one. It runs the _python_ script that captures the images from the _camera feed_ seen in **screen 2**, analyses the images and crops the _found faces_ displayed in **screen 3**. Screen **1** is showing the terminal for the script running, that way people would realise what is going on.

All these images are stored in a hard drive connected directly to **A**. To be able to have the images read by other computers and be able to make use of them, we had a _switch_ that would connect all the machines together.

_something to note here is that, because of your lack of experience, and even shorter time frame we needed to make sure all the system was as bulletproof as possible, to do so we had multiple simple programs running, instead of a big ~~ass~~ one that would do everything._

The only thing computer **B** would do was run a _python_ and _node script;_ a web server, the server and the .py script would be listening for newly added files to the directory where the cropped images were posted. This server would take the _path_ of this new face, and _send it_ to the clients, this would display it in two different ways; **screen 3** would show _only_ the last found face, and **screen 4** would add it to an _array of images_ creating a compilation of all the faces found since connected.

![](/img/thumnailsBlog/42_14.gif)

The **python** script would be connected through serial to an arduino that would trigger a set of LED strips and light up the _"tree"_ and consequently bringing it to life.

Machine **C** and its screens were a different camera feed and two processing sketches that would apply effects to the footage.

![](/img/thumnailsBlog/42_2.png){: .proj_img1024}
_the lights in action._{: .caption1024}

## location/setup

[<img src="/img/thumnailsBlog/42_15.png">](https://www.google.com/maps/@40.7317488,-73.9914357,3a,69.8y,329.73h,99.27t/data=!3m6!1e1!3m4!1sU3slBzhqyEjTEL4479A0Kg!2e0!7i16384!8i8192)
_corner between 11st and Broadway_{: .caption65}

![](/img/thumnailsBlog/42_3.png){: .proj_img1024}
_the final version of this project gave us great moments._{: .caption1024}


![](/img/thumnailsBlog/42_5.png){: .proj_img1024}

![](/img/thumnailsBlog/42_6.png){: .proj_img1024}

---

### links that either, you should check out, or have been mentioned in this post

- [Regina Cantú](http://www.reginacantu.com/)
- [Alexia Kyriakopoulos](http://www.alexiak.com)
- [Ari Meliciano](http://www.ariciano.com)
- [NYU Broadway Windows](https://www.google.com/maps/place/40°43'54.9%22N+73°59'29.7%22W/@40.731923,-73.992312,18z/data=!3m1!4b1!4m6!3m5!1s0x0:0x0!7e2!8m2!3d40.7319232!4d-73.9915898)